{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c55c49-c306-4d8f-9159-52b39b4f6a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import pickle\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from mrmr import mrmr_regression as mrmr_reg\n",
    "random_seed = 10\n",
    "np.set_printoptions(suppress=True)\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fd3a13-37a6-484f-9df5-242a85dea0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading in gene set score \n",
    "set_df = pd.read_csv('set_scores.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea51cd39-9c96-4fd6-bf1d-26a921f9bf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subsetting gene set PRS scores only\n",
    "prs_info = set_df.iloc[:,:65]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a52be1-7f77-4ca9-b540-69e01f4f4730",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting Gene Set Correlation Matrix\n",
    "sns.set_style('darkgrid', {'axes.linewidth': 1, 'axes.edgecolor':'black'})\n",
    "corr_matrix = prs_info.corr().round(2)\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "sns.set(rc = {'figure.figsize':(15,8)})\n",
    "corr_plot = sns.heatmap(corr_matrix, vmax=1, vmin=-1, center=0, cmap='viridis', mask=mask)\n",
    "corr_plot.set_title('Gene Set Correlation Matrix', weight='bold')\n",
    "corr_plot.set_xlabel(\"\", weight='bold')\n",
    "corr_plot.set_ylabel(\"\", weight='bold')\n",
    "sns.set(font_scale = 1.6)\n",
    "sns.despine(top=False, right=False, left=False, bottom=False)\n",
    "plt.show()\n",
    "corr_plot.figure.savefig(\"gene_set_corr.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ad5a5b-d8d4-4815-a0e9-35d0ae1b5aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading in each datasets for each round of 5-fold CV \n",
    "#NOTE: PCs for validation sets are projections onto components from training folds (these are pre-computed)\n",
    "folds_1 = pd.read_csv('folds_1.csv')\n",
    "folds_2 = pd.read_csv('folds_2.csv')\n",
    "folds_3 = pd.read_csv('folds_3.csv')\n",
    "folds_4 = pd.read_csv('folds_4.csv')\n",
    "folds_5 = pd.read_csv('folds_5.csv')\n",
    "\n",
    "folds_list = [folds_1, folds_2, folds_3, folds_4, folds_5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf2daff-aaa4-41f5-966d-eb54fd73cb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Appending Gene Set Scores to each of the datasets \n",
    "\n",
    "#Dict to store updated folds\n",
    "fold_dict = {'fold_1': 0, 'fold_2':0, 'fold_3':0, 'fold_4':0, 'fold_5':0}\n",
    "\n",
    "for i, fold in enumerate(folds_list):\n",
    "    #Retaining only PCs\n",
    "    fold_pcs = fold[['PC1','PC2', 'PC3', 'PC4', 'PC5']]\n",
    "    temp_data = pd.concat([fold_pcs, set_df], axis = 1)\n",
    "    fold_str = f'fold_{i+1}'\n",
    "    fold_dict[fold_str] = temp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87836887-87ea-4b12-9e8a-c9bf7c852270",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in IDs for train-val folds\n",
    "#Dict to store validation fold ids\n",
    "val_id_dict = {'val_1': 0, 'val_2':0, 'val_3':0, 'val_4':0, 'val_5':0}\n",
    "\n",
    "for i in range(5):\n",
    "    #Adding validation set IDs to dictionary for each CV round\n",
    "    id_str = f'val_{i+1}'\n",
    "    val_id_dict[id_str] = pd.read_csv(f'fold_{i+1}.txt', header=None).iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06b0dff-1fd8-4341-b34d-35ec8f017a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating data and spits for each separate round of 5-fold CV\n",
    "x_1, y_1 = fold_dict['fold_1'].iloc[:,:-1], fold_dict['fold_1'].iloc[:,-1]\n",
    "split_1 = [0 if x in val_id_dict['val_1'].values else -1 for x in x_1.index]\n",
    "ps_1 = PredefinedSplit(split_1)\n",
    "\n",
    "x_2, y_2 = fold_dict['fold_2'].iloc[:,:-1], fold_dict['fold_2'].iloc[:,-1]\n",
    "split_2 = [0 if x in val_id_dict['val_2'].values else -1 for x in x_2.index]\n",
    "ps_2 = PredefinedSplit(split_2)\n",
    "\n",
    "x_3, y_3 = fold_dict['fold_3'].iloc[:,:-1], fold_dict['fold_3'].iloc[:,-1]\n",
    "split_3 = [0 if x in val_id_dict['val_3'].values else -1 for x in x_3.index]\n",
    "ps_3 = PredefinedSplit(split_3)\n",
    "\n",
    "x_4, y_4 = fold_dict['fold_4'].iloc[:,:-1], fold_dict['fold_4'].iloc[:,-1]\n",
    "split_4 = [0 if x in val_id_dict['val_4'].values else -1 for x in x_4.index]\n",
    "ps_4 = PredefinedSplit(split_4)\n",
    "\n",
    "x_5, y_5 = fold_dict['fold_5'].iloc[:,:-1], fold_dict['fold_5'].iloc[:,-1]\n",
    "split_5 = [0 if x in val_id_dict['val_5'].values else -1 for x in x_5.index]\n",
    "ps_5 = PredefinedSplit(split_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72544940-4377-4bf2-97f8-94bfdefe0018",
   "metadata": {},
   "source": [
    "$Feature Selection$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636c0d67-db8f-4194-b443-d83778eba8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Return list of top 10, 15, 20, 25, 30 features from MRMR feature selection\n",
    "def get_top_features(x, y, val_ids):\n",
    "    \n",
    "    #Removing principal components\n",
    "    new_data = x.iloc[:,5:]\n",
    "    \n",
    "    #Top 50 features using training data only\n",
    "    top_features = mrmr_reg(new_data.loc[~new_data.index.isin(val_ids.values)], y.loc[~y.index.isin(val_ids.values)].values, K=50)\n",
    "    \n",
    "    #Subsetting based on top N features\n",
    "    x_10 = pd.concat([new_data.loc[:,top_features[:10]], x.iloc[:,:5]], axis=1)\n",
    "    x_15 = pd.concat([new_data.loc[:,top_features[:15]], x.iloc[:,:5]], axis=1)\n",
    "    x_20 = pd.concat([new_data.loc[:,top_features[:20]], x.iloc[:,:5]], axis=1)\n",
    "    x_25 = pd.concat([new_data.loc[:,top_features[:25]], x.iloc[:,:5]], axis=1)\n",
    "    x_30 = pd.concat([new_data.loc[:,top_features[:30]], x.iloc[:,:5]], axis=1)\n",
    "    \n",
    "    return [x_10, x_15, x_20, x_25, x_30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0753b6e-73d1-46f5-8e53-b964eec9767c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of feature sets to test\n",
    "num_feats = ['10', '15', '20', '25', '30']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8592e903-8a45-4962-aad2-8da3d3290372",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get list of top features for each dataset for each round of CV\n",
    "x1_list = get_top_features(x_1, y_1, val_id_dict['val_1'])\n",
    "x2_list = get_top_features(x_2, y_2, val_id_dict['val_2'])\n",
    "x3_list = get_top_features(x_3, y_3, val_id_dict['val_3'])\n",
    "x4_list = get_top_features(x_4, y_4, val_id_dict['val_4'])\n",
    "x5_list = get_top_features(x_5, y_5, val_id_dict['val_5'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97f4298-d3ac-4371-b6ef-1072f5feafa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to return dataframe of 5-fold CV results\n",
    "def cv_results(result_list):\n",
    "    result_dict = {'params':result_list[0]['params']}\n",
    "    \n",
    "    #Storing results from each CV round\n",
    "    for i, result in enumerate(result_list):\n",
    "        result_dict[f'split_{i}_neg_mse'] = result_list[i]['mean_test_neg_mean_squared_error']\n",
    "        result_dict[f'split_{i}_r2'] = result_list[i]['mean_test_r2']\n",
    "    \n",
    "    #Calculating mean and std of results\n",
    "    num_res = len(result_dict['params'])\n",
    "                  \n",
    "    mean_mse_list = []\n",
    "    sd_mse_list = []\n",
    "    mean_r2_list = []\n",
    "    sd_r2_list = []\n",
    "                  \n",
    "    for j in range(num_res):\n",
    "        mse_vals = [result_dict['split_0_neg_mse'][j], result_dict['split_1_neg_mse'][j], result_dict['split_2_neg_mse'][j],\n",
    "                           result_dict['split_3_neg_mse'][j], result_dict['split_4_neg_mse'][j]]\n",
    "        r2_vals = [result_dict['split_0_r2'][j], result_dict['split_1_r2'][j], result_dict['split_2_r2'][j],\n",
    "                           result_dict['split_3_r2'][j], result_dict['split_4_r2'][j]]\n",
    "        \n",
    "        mean_mse = np.mean(mse_vals)\n",
    "        sd_mse = np.std(mse_vals)\n",
    "        mean_r2 = np.mean(r2_vals)\n",
    "        sd_r2 = np.std(r2_vals)\n",
    "        \n",
    "        mean_mse_list.append(mean_mse)\n",
    "        sd_mse_list.append(sd_mse)\n",
    "        mean_r2_list.append(mean_r2)\n",
    "        sd_r2_list.append(sd_r2)\n",
    "        \n",
    "    result_dict['mean_neg_mse'] = mean_mse_list\n",
    "    result_dict['sd_neg_mse'] = sd_mse_list\n",
    "    result_dict['mean_r2'] = mean_r2_list\n",
    "    result_dict['sd_r2'] = sd_r2_list\n",
    "    \n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a8dcb8-84e3-4312-952d-d2119da15345",
   "metadata": {},
   "source": [
    "$Feature Scaling$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54d484f-c221-459b-9073-1ee201b1f61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns list of scaled dataset for each set of features\n",
    "def scale_data(x, y, val_dict, val_id):\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    #Rescaling sex\n",
    "    x['sex'] = x['sex'].apply(lambda x: -1 if x == 1 else 1)\n",
    "    \n",
    "    #Scaling remaining features\n",
    "    x_new = scaler.fit_transform(x.iloc[:,:70])\n",
    "    x_new = pd.DataFrame(data=x_new, columns=scaler.get_feature_names_out() )\n",
    "    \n",
    "    #Joining datasets\n",
    "    x_new = pd.concat([x_new, x.iloc[:,-1]], axis=1)\n",
    "    \n",
    "    #Get list of datasets with top 10, 15, 20, 25, and 30 features\n",
    "    x_list = get_top_features(x_new, y, val_dict[val_id])\n",
    "    \n",
    "    return x_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81ac200-65ae-4840-84a0-112f1f192e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating scaled datasets\n",
    "x1_scale = scale_data(x_1, y_1, val_id_dict, 'val_1')\n",
    "x2_scale = scale_data(x_2, y_2, val_id_dict, 'val_2')\n",
    "x3_scale = scale_data(x_3, y_3, val_id_dict, 'val_3')\n",
    "x4_scale = scale_data(x_4, y_4, val_id_dict, 'val_4')\n",
    "x5_scale = scale_data(x_5, y_5, val_id_dict, 'val_5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab9846d-c782-4a5d-aca1-70f85dc46c73",
   "metadata": {},
   "source": [
    "$Elastic Net Regression$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba5300a-1937-4e94-a5e7-825f79cf7eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameter Search Grid for Elastic Net Model\n",
    "param_grid_enet = {\"alpha\": [0.0001, 0.001, 0.01, 0.1, 1, 10, 100],\n",
    "              \"l1_ratio\": [.1, .5, .7, .9, .95, .99, 1]}\n",
    "\n",
    "#Defining Model\n",
    "eNet = ElasticNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e1a613-2948-4268-ae3f-5e129cb90734",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reduced Feature Sets\n",
    "#Carry out each round of 5-fold CV separately to ensure pre-defined folds are used\n",
    "#Dict to store results for each feature set\n",
    "enet_result_dict = {'10':0, '15':0, '20':0, '25':0, '30':0}\n",
    "\n",
    "#Get results for each reduced set of features\n",
    "for i, feats in enumerate(num_feats):\n",
    "    grid_enet_1 = GridSearchCV(eNet, param_grid_enet, scoring=['neg_mean_squared_error','r2'], cv=ps_1, refit=False)\n",
    "    grid_enet_1.fit(x1_list[i], y_1)\n",
    "\n",
    "    grid_enet_2 = GridSearchCV(eNet, param_grid_enet, scoring=['neg_mean_squared_error','r2'], cv=ps_2, refit=False)\n",
    "    grid_enet_2.fit(x2_list[i], y_2)\n",
    "\n",
    "    grid_enet_3 = GridSearchCV(eNet, param_grid_enet, scoring=['neg_mean_squared_error','r2'], cv=ps_3, refit=False)\n",
    "    grid_enet_3.fit(x3_list[i], y_3)\n",
    "\n",
    "    grid_enet_4 = GridSearchCV(eNet, param_grid_enet, scoring=['neg_mean_squared_error','r2'], cv=ps_4, refit=False)\n",
    "    grid_enet_4.fit(x4_list[i], y_4)\n",
    "\n",
    "    grid_enet_5 = GridSearchCV(eNet, param_grid_enet, scoring=['neg_mean_squared_error','r2'], cv=ps_5, refit=False)\n",
    "    grid_enet_5.fit(x5_list[i], y_5)\n",
    "\n",
    "    enet_result_list = [grid_enet_1.cv_results_, grid_enet_2.cv_results_, grid_enet_3.cv_results_, grid_enet_4.cv_results_, \n",
    "                        grid_enet_5.cv_results_]\n",
    "\n",
    "    enet_result_dict[feats] = cv_results(enet_result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cab373-a8c0-4071-9373-f9e661ef13c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Full feature set\n",
    "grid_enet_1 = GridSearchCV(eNet, param_grid_enet, scoring=['neg_mean_squared_error','r2'], cv=ps_1, refit=False)\n",
    "grid_enet_1.fit(x_1, y_1)\n",
    "\n",
    "grid_enet_2 = GridSearchCV(eNet, param_grid_enet, scoring=['neg_mean_squared_error','r2'], cv=ps_2, refit=False)\n",
    "grid_enet_2.fit(x_2, y_2)\n",
    "\n",
    "grid_enet_3 = GridSearchCV(eNet, param_grid_enet, scoring=['neg_mean_squared_error','r2'], cv=ps_3, refit=False)\n",
    "grid_enet_3.fit(x_3, y_3)\n",
    "\n",
    "grid_enet_4 = GridSearchCV(eNet, param_grid_enet, scoring=['neg_mean_squared_error','r2'], cv=ps_4, refit=False)\n",
    "grid_enet_4.fit(x_4, y_4)\n",
    "\n",
    "grid_enet_5 = GridSearchCV(eNet, param_grid_enet, scoring=['neg_mean_squared_error','r2'], cv=ps_5, refit=False)\n",
    "grid_enet_5.fit(x_5, y_5)\n",
    "\n",
    "enet_result_list = [grid_enet_1.cv_results_, grid_enet_2.cv_results_, grid_enet_3.cv_results_, grid_enet_4.cv_results_, \n",
    "                    grid_enet_5.cv_results_]\n",
    "\n",
    "enet_full_set_results = cv_results(enet_result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8006af7a-a0a6-4538-a9f9-ba02f1a934dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('enet_full_set_results_final.pkl', 'wb') as f:\n",
    "    pickle.dump(enet_full_set_results, f)\n",
    "    \n",
    "with open('enet_feat_sets_results_final.pkl', 'wb') as f:\n",
    "    pickle.dump(enet_result_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb2fa2f-6b1f-431e-9a8d-1c110544f225",
   "metadata": {},
   "source": [
    "$XGBoost$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c6dab8-74af-4b1e-a706-ce66833056be",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_xgb = {'n_estimators': [50, 100, 250, 500, 1000, 1500],\n",
    "    'learning_rate': [0.2, 0.05, 0.01, 0.001],\n",
    "    'max_depth': [3, 6, 8],\n",
    "    'min_child_weight': [3, 6],\n",
    "    'gamma': [0.2, 0.5],\n",
    "    'subsample': [0.5, 0.8],\n",
    "    'random_state': [1],\n",
    "    'colsample_bytree':[0.5,0.8],\n",
    "    'n_jobs': [-1],\n",
    "    'objective': ['reg:squarederror']}\n",
    "\n",
    "xgb_model = XGBRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de5f6d8-f2be-4711-9df9-85232ebe1877",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reduced Feature Sets\n",
    "#Carry out each round of 5-fold CV separately to ensure pre-defined folds are used\n",
    "#Dict to store results for each feature set\n",
    "xgb_result_dict = {'10':0, '15':0, '20':0, '25':0, '30':0}\n",
    "\n",
    "#Get results for each reduced set of features\n",
    "for i, feats in enumerate(num_feats):\n",
    "    \n",
    "    print(f'Feature set {i+1}/5')\n",
    "    grid_xgb_1 = GridSearchCV(xgb_model, param_grid_xgb, scoring=['neg_mean_squared_error','r2'], cv=ps_1, refit=False)\n",
    "    grid_xgb_1.fit(x1_list[i], y_1)\n",
    "\n",
    "    grid_xgb_2 = GridSearchCV(xgb_model, param_grid_xgb, scoring=['neg_mean_squared_error','r2'], cv=ps_2, refit=False)\n",
    "    grid_xgb_2.fit(x2_list[i], y_2)\n",
    "\n",
    "    grid_xgb_3 = GridSearchCV(xgb_model, param_grid_xgb, scoring=['neg_mean_squared_error','r2'], cv=ps_3, refit=False)\n",
    "    grid_xgb_3.fit(x3_list[i], y_3)\n",
    "\n",
    "    grid_xgb_4 = GridSearchCV(xgb_model, param_grid_xgb, scoring=['neg_mean_squared_error','r2'], cv=ps_4, refit=False)\n",
    "    grid_xgb_4.fit(x4_list[i], y_4)\n",
    "\n",
    "    grid_xgb_5 = GridSearchCV(xgb_model, param_grid_xgb, scoring=['neg_mean_squared_error','r2'], cv=ps_5, refit=False)\n",
    "    grid_xgb_5.fit(x5_list[i], y_5)\n",
    "\n",
    "    xgb_result_list = [grid_xgb_1.cv_results_, grid_xgb_2.cv_results_, grid_xgb_3.cv_results_, grid_xgb_4.cv_results_, \n",
    "                        grid_xgb_5.cv_results_]\n",
    "\n",
    "    xgb_result_dict[feats] = cv_results(xgb_result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df73673-9b05-4919-b126-f59b00e42177",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('xgb_results_final.pkl', 'wb') as f:\n",
    "    pickle.dump(xgb_result_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70823fa-a9ed-429b-a404-815f7e48c09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Further tuning of regularisation terms\n",
    "param_grid_xgb_reg = {'n_estimators': [500, 1000, 1500],\n",
    "    'learning_rate': [0.05, 0.01, 0.001],\n",
    "    'max_depth': [3, 6],\n",
    "    'min_child_weight': [3, 6],\n",
    "    'gamma': [0.2, 0.5],\n",
    "    'subsample': [0.5, 0.8],\n",
    "    'random_state': [1],\n",
    "    'colsample_bytree':[0.5, 0.8],\n",
    "    'n_jobs': [-1],\n",
    "    'objective': ['reg:squarederror'],\n",
    "    'reg_lambda':[0.01, 0.1, 1, 10, 100],\n",
    "    'reg_alpha':[0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "xgb_model_reg = XGBRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87189ab1-2766-4b34-bc92-bd61640d3309",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_xgb_1_reg = GridSearchCV(xgb_model_reg, param_grid_xgb_reg, scoring=['neg_mean_squared_error','r2'], cv=ps_1, refit=False)\n",
    "grid_xgb_1_reg.fit(x_1, y_1)\n",
    "\n",
    "grid_xgb_2_reg = GridSearchCV(xgb_model_reg, param_grid_xgb_reg, scoring=['neg_mean_squared_error','r2'], cv=ps_2, refit=False)\n",
    "grid_xgb_2_reg.fit(x_2, y_2)\n",
    "\n",
    "grid_xgb_3_reg = GridSearchCV(xgb_model_reg, param_grid_xgb_reg, scoring=['neg_mean_squared_error','r2'], cv=ps_3, refit=False)\n",
    "grid_xgb_3_reg.fit(x_3, y_3)\n",
    "\n",
    "grid_xgb_4_reg = GridSearchCV(xgb_model_reg, param_grid_xgb_reg, scoring=['neg_mean_squared_error','r2'], cv=ps_4, refit=False)\n",
    "grid_xgb_4_reg.fit(x_4, y_4)\n",
    "\n",
    "grid_xgb_5_reg = GridSearchCV(xgb_model_reg, param_grid_xgb_reg, scoring=['neg_mean_squared_error','r2'], cv=ps_5, refit=False)\n",
    "grid_xgb_5_reg.fit(x_5, y_5)\n",
    "\n",
    "xgb_result_list_reg = [grid_xgb_1_reg.cv_results_, grid_xgb_2_reg.cv_results_, grid_xgb_3_reg.cv_results_, grid_xgb_4_reg.cv_results_, \n",
    "                    grid_xgb_5_reg.cv_results_]\n",
    "\n",
    "xgb_full_set_results = cv_results(xgb_result_list_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7183d138-3e48-4971-b481-6b7af01451ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('xgb_results_final_full_set.pkl', 'wb') as f:\n",
    "    pickle.dump(xgb_full_set_results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8dd91df-c649-40b6-a884-67da5f7cc066",
   "metadata": {},
   "source": [
    "$Random Forest$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f583d53-a93c-4fcd-b7c3-4e07120fd018",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Selection Grid\n",
    "param_grid_rf = {'n_estimators': [100, 250, 500, 1000, 1500, 2000],\n",
    "                 'criterion':['squared_error'],\n",
    "                 'max_features':[2, 4, 6, 8],\n",
    "                 'max_depth':[int(x) for x in np.linspace(10, 100, num = 10)],\n",
    "                 'min_samples_split':[2, 5, 10],\n",
    "                 'min_samples_leaf':[1, 2, 4]\n",
    "                 }\n",
    "\n",
    "rf_model = RandomForestRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8ef2d0-90cb-4c5f-98c5-7c9bb062eecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reduced Feature Sets\n",
    "#Carry out each round of 5-fold CV separately to ensure pre-defined folds are used\n",
    "#Dict to store results for each feature set\n",
    "rf_result_dict = {'10':0, '15':0, '20':0, '25':0, '30':0}\n",
    "\n",
    "#Get results for each reduced set of features\n",
    "for i, feats in enumerate(num_feats):\n",
    "    \n",
    "    print(f'Feature set {i+1}/5')\n",
    "    \n",
    "    #Carry out each round of 5-fold CV separately to ensure pre-defined folds are used\n",
    "    grid_rf_1 = GridSearchCV(rf_model, param_grid_rf, scoring=['neg_mean_squared_error','r2'], cv=ps_1, refit=False)\n",
    "    grid_rf_1.fit(x1_list[i], y_1)\n",
    "\n",
    "    grid_rf_2 = GridSearchCV(rf_model, param_grid_rf, scoring=['neg_mean_squared_error','r2'], cv=ps_2, refit=False)\n",
    "    grid_rf_2.fit(x2_list[i], y_2)\n",
    "\n",
    "    grid_rf_3 = GridSearchCV(rf_model, param_grid_rf, scoring=['neg_mean_squared_error','r2'], cv=ps_3, refit=False)\n",
    "    grid_rf_3.fit(x3_list[i], y_3)\n",
    "\n",
    "    grid_rf_4 = GridSearchCV(rf_model, param_grid_rf, scoring=['neg_mean_squared_error','r2'], cv=ps_4, refit=False)\n",
    "    grid_rf_4.fit(x4_list[i], y_4)\n",
    "\n",
    "    grid_rf_5 = GridSearchCV(rf_model, param_grid_rf, scoring=['neg_mean_squared_error','r2'], cv=ps_5, refit=False)\n",
    "    grid_rf_5.fit(x5_list[i], y_5)\n",
    "    \n",
    "    rf_result_list = [grid_rf_1.cv_results_, grid_rf_2.cv_results_, grid_rf_3.cv_results_, grid_rf_4.cv_results_, \n",
    "                        grid_rf_5.cv_results_]\n",
    "\n",
    "    rf_result_dict[feats] = cv_results(rf_result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c84e3f-dcca-4619-ac49-0c8fab27c9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('rf_results_final.pkl', 'wb') as f:\n",
    "    pickle.dump(rf_result_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0c43a3-7f4c-44a7-8d12-a31d3bbf7183",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Full Feature Set Grid\n",
    "param_grid_rf_full = {'n_estimators': [100, 250, 500, 1000, 1500, 2000],\n",
    "                 'criterion':['squared_error'],\n",
    "                 'max_features':[4, 6, 8, 10],\n",
    "                 'max_depth':[int(x) for x in np.linspace(10, 100, num = 10)],\n",
    "                 'min_samples_split':[2, 5, 10],\n",
    "                 'min_samples_leaf':[1, 2, 4]\n",
    "                 }\n",
    "\n",
    "rf_model = RandomForestRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1614bb5a-86e1-4d0b-83f3-8b23e7749fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Full Feature Set\n",
    "#Carry out each round of 5-fold CV separately to ensure pre-defined folds are used\n",
    "\n",
    "#Get results for each reduced set of features\n",
    "#Carry out each round of 5-fold CV separately to ensure pre-defined folds are used\n",
    "grid_rf_full_set_1 = GridSearchCV(rf_model, param_grid_rf_full, scoring=['neg_mean_squared_error','r2'], cv=ps_1, refit=False)\n",
    "grid_rf_full_set_1.fit(x_1, y_1)\n",
    "\n",
    "grid_rf_full_set_2 = GridSearchCV(rf_model, param_grid_rf_full, scoring=['neg_mean_squared_error','r2'], cv=ps_2, refit=False)\n",
    "grid_rf_full_set_2.fit(x_2, y_2)\n",
    "\n",
    "grid_rf_full_set_3 = GridSearchCV(rf_model, param_grid_rf_full, scoring=['neg_mean_squared_error','r2'], cv=ps_3, refit=False)\n",
    "grid_rf_full_set_3.fit(x_3, y_3)\n",
    "\n",
    "grid_rf_full_set_4 = GridSearchCV(rf_model, param_grid_rf_full, scoring=['neg_mean_squared_error','r2'], cv=ps_4, refit=False)\n",
    "grid_rf_full_set_4.fit(x_4, y_4)\n",
    "\n",
    "grid_rf_full_set_5 = GridSearchCV(rf_model, param_grid_rf_full, scoring=['neg_mean_squared_error','r2'], cv=ps_5, refit=False)\n",
    "grid_rf_full_set_5.fit(x_5, y_5)\n",
    "\n",
    "rf_full_set_result_list = [grid_rf_full_set_1.cv_results_, grid_rf_full_set_2.cv_results_, grid_rf_full_set_3.cv_results_, grid_rf_full_set_4.cv_results_, \n",
    "                    grid_rf_full_set_5.cv_results_]\n",
    "\n",
    "rf_full_set_results = cv_results(rf_full_set_result_list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7623ebb-f3bd-4cba-bf1e-1b8322a8b907",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('rf_results_full_set_final.pkl', 'wb') as f:\n",
    "    pickle.dump(rf_full_set_results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ee22f5-a6b0-4d9b-9e10-b85f65f66ac1",
   "metadata": {},
   "source": [
    "$Support Vector Machine$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b520c96-42e9-4318-b6c7-af3d246d20a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_svm = {'kernel': ['linear', 'poly', 'rbf'],\n",
    "                 'degree':[2,3,4],\n",
    "                 'gamma':['auto', 'scale'],\n",
    "                 'C':[0.1, 1, 10, 100, 1000],\n",
    "                 'epsilon':[0, 0.01, 0.1, 0.5, 1, 2, 5]\n",
    "                 }\n",
    "\n",
    "svm_model = SVR()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cb00f7-0ec5-48ee-a0d9-f64f41f1688c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reduced Feature Sets\n",
    "#Carry out each round of 5-fold CV separately to ensure pre-defined folds are used\n",
    "#Dict to store results for each feature set\n",
    "svm_result_dict = {'10':0, '15':0, '20':0, '25':0, '30':0}\n",
    "\n",
    "#Get results for each reduced set of features\n",
    "for i, feats in enumerate(num_feats):\n",
    "    \n",
    "    print(f'Feature set {i+1}/5')\n",
    "    \n",
    "    #Carry out each round of 5-fold CV separately to ensure pre-defined folds are used\n",
    "    grid_svm_1 = GridSearchCV(svm_model, param_grid_svm, scoring=['neg_mean_squared_error','r2'], cv=ps_1, refit=False)\n",
    "    grid_svm_1.fit(x1_scale[i], y_1)\n",
    "\n",
    "    grid_svm_2 = GridSearchCV(svm_model, param_grid_svm, scoring=['neg_mean_squared_error','r2'], cv=ps_2, refit=False)\n",
    "    grid_svm_2.fit(x2_scale[i], y_2)\n",
    "\n",
    "    grid_svm_3 = GridSearchCV(svm_model, param_grid_svm, scoring=['neg_mean_squared_error','r2'], cv=ps_3, refit=False)\n",
    "    grid_svm_3.fit(x3_scale[i], y_3)\n",
    "\n",
    "    grid_svm_4 = GridSearchCV(svm_model, param_grid_svm, scoring=['neg_mean_squared_error','r2'], cv=ps_4, refit=False)\n",
    "    grid_svm_4.fit(x4_scale[i], y_4)\n",
    "\n",
    "    grid_svm_5 = GridSearchCV(svm_model, param_grid_svm, scoring=['neg_mean_squared_error','r2'], cv=ps_5, refit=False)\n",
    "    grid_svm_5.fit(x5_scale[i], y_5)\n",
    "    \n",
    "    svm_result_list = [grid_svm_1.cv_results_, grid_svm_2.cv_results_, grid_svm_3.cv_results_, grid_svm_4.cv_results_, \n",
    "                        grid_svm_5.cv_results_]\n",
    "\n",
    "    svm_result_dict[feats] = cv_results(svm_result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632e2070-9027-4649-bfe3-049245d9f87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('svm_results_final.pkl', 'wb') as f:\n",
    "    pickle.dump(svm_result_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6b2427-7d3c-4484-958c-49eed682a9f2",
   "metadata": {},
   "source": [
    "$Data Preparation$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0df058-6530-4c65-bfb1-afb9e7a33d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading in test data\n",
    "test_data = pd.read_csv('final_test_data.csv')\n",
    "test_data = test_data[['height', 'Sex','LDPRED2_0.841982_0.3267_TRUE', 'PC1', 'PC2', 'PC3', 'PC4', 'PC5']]\n",
    "test_data.columns = ['height', 'Sex', 'full_prs', 'PC1', 'PC2', 'PC3', 'PC4', 'PC5']\n",
    "test_data.drop(['height', 'Sex', 'full_prs'], axis=1, inplace=True)\n",
    "test_gene_sets = pd.read_csv('test_data_set_scores.csv')\n",
    "full_test_data = pd.concat([test_data, test_gene_sets], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6543bb19-27ad-4d6e-b11b-0e0d9b56b9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading in training data\n",
    "train_data = pd.read_csv('full_train_set.csv')\n",
    "train_data = train_data[['height', 'Sex','LDPRED2_0.841982_0.3267_TRUE', 'PC1', 'PC2', 'PC3', 'PC4', 'PC5']]\n",
    "train_data.columns = ['height', 'Sex', 'full_prs', 'PC1', 'PC2', 'PC3', 'PC4', 'PC5']\n",
    "train_data.drop(['height', 'Sex','full_prs'], axis=1, inplace=True)\n",
    "train_gene_sets = pd.read_csv('set_scores.csv')\n",
    "full_train_data = pd.concat([train_data, train_gene_sets], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370de856-8a27-42e0-ab9c-9170ccbb3f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining target and features\n",
    "X_train, y_train = full_train_data.iloc[:,:-1], full_train_data.iloc[:,-1]\n",
    "X_test, y_test = full_test_data.iloc[:,:-1], full_test_data.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22751a81-9996-4816-9e73-87dfde27073f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to evaluate model based on R2 and MSE\n",
    "def eval_model(y_preds, y_true):\n",
    "    r2 = r2_score(y_true, y_preds)\n",
    "    mse = mean_squared_error(y_true, y_preds)\n",
    "    \n",
    "    print(f'Test R2 Score: {r2}')\n",
    "    print(f'Test MSE: {mse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d638b95-f5e6-406b-92dc-eba5c387b2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to scale X data\n",
    "def final_scale_data(x):\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    #Rescaling sex\n",
    "    x['sex'] = x['sex'].apply(lambda x: -1 if x == 1 else 1)\n",
    "    \n",
    "    #Scaling remaining features\n",
    "    x_new = scaler.fit_transform(x.iloc[:,:70])\n",
    "    x_new = pd.DataFrame(data=x_new, columns=scaler.get_feature_names_out() )\n",
    "    \n",
    "    #Joining datasets\n",
    "    x_new = pd.concat([x_new, x.iloc[:,-1]], axis=1)\n",
    "    \n",
    "    return x_new "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0e792c-ac54-4348-9308-c52d40581288",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaled X data\n",
    "X_train_scaled = final_scale_data(X_train)\n",
    "X_test_scaled = final_scale_data(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e19489c-6e1c-466d-8c92-0982e031f84b",
   "metadata": {},
   "source": [
    "$RuleFit$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920fc038-9d6b-4408-bcbb-6395223a0c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rulefit_enet as rf_enet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9478d554-e4a3-473a-9c58-0ce151e96f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5-fold Cross-Validation of Rule Fit Model\n",
    "rfit_1 = rf_enet.RuleFit(tree_generator = RandomForestRegressor(n_estimators=500), cv=ps_1)\n",
    "rfit_1.fit(x_1.values, y_1, feature_names=x_1.columns)\n",
    "\n",
    "rfit_2 = rf_enet.RuleFit(tree_generator = RandomForestRegressor(n_estimators=500), cv=ps_2)\n",
    "rfit_2.fit(x_2.values, y_2, feature_names=x_2.columns)\n",
    "\n",
    "rfit_3 = rf_enet.RuleFit(tree_generator = RandomForestRegressor(n_estimators=500), cv=ps_3)\n",
    "rfit_3.fit(x_3.values, y_3, feature_names=x_3.columns)\n",
    "\n",
    "rfit_4 = rf_enet.RuleFit(tree_generator = RandomForestRegressor(n_estimators=500), cv=ps_4)\n",
    "rfit_4.fit(x_4.values, y_4, feature_names=x_4.columns)\n",
    "\n",
    "rfit_5 = rf_enet.RuleFit(tree_generator = RandomForestRegressor(n_estimators=500), cv=ps_5)\n",
    "rfit_5.fit(x_5.values, y_5, feature_names=x_5.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afde32b-cca2-4090-a2bf-6c23010f1d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating dataframe of 5-fold CV results for RuleFit model\n",
    "num_alphas = rfit_1.enetcv.mse_path_.shape[1]\n",
    "num_l1_ratio = rfit_1.enetcv.mse_path_.shape[0]\n",
    "l1_ratios = [.1, .5, .7, .9, .95, .99, 1]\n",
    "alphas = rfit_1.enetcv.alphas_\n",
    "rfit_res_dict = dict(keys=['fold_1', 'fold_2', 'fold_3', 'fold_4', 'fold_5', 'alpha', 'l1_ratio'])\n",
    "f1, f2, f3, f4, f5 = [], [], [], [], []\n",
    "alpha_vals, l1_ratio_vals = [],[]\n",
    "\n",
    "for j in range(num_l1_ratio):\n",
    "    for i in range(num_alphas):\n",
    "        alpha_vals.append(alphas[j][i])\n",
    "        l1_ratio_vals.append(l1_ratios[j])\n",
    "        f1.append(rfit_1.enetcv.mse_path_[j][i])\n",
    "        f2.append(rfit_2.enetcv.mse_path_[j][i])\n",
    "        f3.append(rfit_3.enetcv.mse_path_[j][i])\n",
    "        f4.append(rfit_4.enetcv.mse_path_[j][i])\n",
    "        f5.append(rfit_5.enetcv.mse_path_[j][i])\n",
    "    \n",
    "rfit_res_dict['fold_1'] = f1\n",
    "rfit_res_dict['fold_2'] = f2\n",
    "rfit_res_dict['fold_3'] = f3\n",
    "rfit_res_dict['fold_4'] = f4\n",
    "rfit_res_dict['fold_5'] = f5\n",
    "rfit_res_dict['alpha'] = alpha_vals\n",
    "rfit_res_dict['l1_ratio'] = l1_ratio_vals\n",
    "\n",
    "rfit_df = pd.DataFrame(columns=['fold_1', 'fold_2', 'fold_3', 'fold_4', 'fold_5', 'alpha', 'l1_ratio'], data=rfit_res_dict)\n",
    "rfit_df['mean_mse'] = rfit_df.apply(lambda x: np.mean([x.fold_1,x.fold_2,x.fold_3,x.fold_4,x.fold_5]),axis=1)\n",
    "rfit_df['sd_mse'] = rfit_df.apply(lambda x: np.std([x.fold_1,x.fold_2,x.fold_3,x.fold_4,x.fold_5]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bb5d1a-b833-45a1-b9cb-c669aa5c1743",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determining optimal value of alpha from 5-fold CV\n",
    "min_rfit_id = np.argmin(rfit_df['mean_mse'])\n",
    "alpha_best = rfit_df['alpha'][min_rfit_id]\n",
    "rfit_df.iloc[min_rfit_id,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a907065c-c5b9-40ce-a05c-f5786a7c9419",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rule fit model using elastic net regression\n",
    "rfit_enet = rf_enet.RuleFit(tree_generator = RandomForestRegressor(n_estimators=100, max_features=4, max_depth=10, min_samples_leaf=1, min_samples_split=2), cv=5)\n",
    "rfit_enet.fit(X_train.values, y_train, feature_names=X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4142165a-4c66-48d5-9a5a-f5a83f60c188",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting and exporting RuleFit features for plotting in R\n",
    "rfit_rules = rfit_enet.get_rules().sort_values(by='importance', ascending=False)\n",
    "rfit_rules.to_csv('rfit_rules.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fc87cb-5a29-4afb-9ed9-646720c555c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting results for optimal parameter values\n",
    "opt_alpha = rfit_enet.enetcv.alpha_\n",
    "opt_lambda = rfit_enet.enetcv.l1_ratio_\n",
    "l1_ratios = [.1, .5, .7, .9, .95, .99, 1]\n",
    "lambda_id = np.where(l1_ratios == opt_lambda)\n",
    "alpha_id = np.where(rfit_enet.enetcv.alphas_[lambda_id] == opt_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133290c2-9afa-463b-b913-27dfd35bc09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MSE values for best parameter set\n",
    "mse_list = rfit_enet.enetcv.mse_path_[lambda_id][alpha_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46102e89-0d80-4c7b-b823-b88f3c4f0812",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mean and SD MSE\n",
    "mean_mse_rfit = np.mean(mse_list)\n",
    "sd_mse_rfit = np.std(mse_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd02ef36-84db-4a2c-a14d-0d14aedbb86f",
   "metadata": {},
   "source": [
    "$Cross-Validation Results$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e30e262-ead7-4e12-8664-7aa864db0d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading in results from models\n",
    "with open(\"svm_results_final.pkl\", \"rb\") as f:\n",
    "    svm_results = pickle.load(f)\n",
    "    \n",
    "with open(\"rf_results_final.pkl\", \"rb\") as f:\n",
    "    rf_results = pickle.load(f)\n",
    "    \n",
    "with open(\"rf_results_full_set_final.pkl\", \"rb\") as f:\n",
    "    rf_full_set_results = pickle.load(f)    \n",
    "    \n",
    "with open(\"xgb_results_final.pkl\", \"rb\") as f:\n",
    "    xgb_results = pickle.load(f)\n",
    "    \n",
    "with open('xgb_results_final_full_set.pkl', 'rb') as f:\n",
    "    xgb_results_reg = pickle.load(f)\n",
    "\n",
    "with open(\"enet_full_set_results_final.pkl\", \"rb\") as f:\n",
    "    enet_results = pickle.load(f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec84cf2a-a0bc-4a6b-a961-fc6ca60e8c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to return top models ordered by R2 for each method\n",
    "def top_models(result_dict):\n",
    "    r2_scores = result_dict['mean_r2']\n",
    "    sd_r2 = result_dict['sd_r2']\n",
    "    mse_scores = result_dict['mean_neg_mse']\n",
    "    sd_mse = result_dict['sd_neg_mse']\n",
    "    params = result_dict['params']\n",
    "    data_tuple = list(zip(r2_scores, sd_r2, mse_scores, sd_mse, params))\n",
    "    results_df = pd.DataFrame(data_tuple, columns=['mean_r2', 'sd_r2', 'mean_mse', 'sd_mse', 'params'])\n",
    "    results_df.sort_values(by='mean_r2', inplace=True, ascending=False)\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881644de-dde1-4bf7-a8fa-9bce4533c57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating dataframes of top 5 models for each method\n",
    "enet_df = top_models(enet_results)\n",
    "xgb_reg_df = top_models(xgb_results_reg)\n",
    "xgb_df = top_models(xgb_results['10'])\n",
    "rf_full_set_df = top_models(rf_full_set_results)\n",
    "rf_df = top_models(rf_results['10'])\n",
    "svm_df = top_models(svm_results['10'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0938bcb1-ab0b-429d-a9db-1c75872696ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Elastic Net 5-fold CV R2\n",
    "print(f'Elastic Net \\nR2: {max(enet_results[\"mean_r2\"])}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f4b86a-e1bf-4765-8c3d-e623f5fb111c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest 5-fold CV R2\n",
    "for feat in num_feats:\n",
    "    print(f'Features: {feat}\\nR2: {max(rf_results[feat][\"mean_r2\"])}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2b37b6-b2c2-4e65-a223-706f7a0353c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGBoost-reg 5-fold CV R2\n",
    "print(f'R2: {max(xgb_results_reg[\"mean_r2\"])}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97afefb-a44a-4392-b62e-5829184bed61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGBoost-feat 5-fold CV R2\n",
    "for feat in num_feats:\n",
    "    print(f'Features: {feat}\\nR2: {max(xgb_results[feat][\"mean_r2\"])}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f47ceb-33aa-43f5-bbe9-6cbae05fe939",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVR 5-fold CV R2\n",
    "for feat in num_feats:\n",
    "    print(f'Features: {feat}\\nR2: {max(svm_results[feat][\"mean_r2\"])}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a56c64c-1185-4612-96be-db5fd48cf8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating dataframe of model R2 scores for each number of features\n",
    "model_list = ['XGB', 'RF', 'SVR']\n",
    "result_list = [xgb_results, rf_results, svm_results]\n",
    "\n",
    "models = []\n",
    "feats = []\n",
    "r2_vals = []\n",
    "r2_sd = []\n",
    "\n",
    "for i, model in enumerate(result_list):\n",
    "    for j, feat in enumerate(num_feats):\n",
    "        models.append(model_list[i])\n",
    "        feats.append(feat)\n",
    "        r2_vals.append(max(model[feat]['mean_r2']))\n",
    "        r2_sd.append(model[feat]['sd_r2'][np.argmax(model[feat]['mean_r2'])])\n",
    "    \n",
    "feat_res_df = pd.DataFrame(list(zip(models, feats, r2_vals, r2_sd)), columns=['model','num_features','r2', 'sd'])\n",
    "feat_res_df.to_csv('results_feat_num.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b273c7-c425-4c12-a419-30acac7d13e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Doing feature selection for full training set\n",
    "top_feats_ordered = mrmr_reg(X_train, y_train, K=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bbf6d6-5245-43e7-9e87-fb301830af1c",
   "metadata": {},
   "source": [
    "$Test Set Evaluation$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d412783-db16-4f21-a4e0-7f2b2aa8c12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest\n",
    "rf_best = RandomForestRegressor(n_estimators=100, criterion='squared_error', max_depth=50,\n",
    "                               min_samples_leaf=1, min_samples_split=10, max_features=6,\n",
    "                               random_state=random_seed)\n",
    "\n",
    "rf_best.fit(X_train.loc[:,top_feats_ordered[:10]], y_train)\n",
    "\n",
    "rf_preds = rf_best.predict(X_test.loc[:,top_feats_ordered[:10]])\n",
    "\n",
    "eval_model(rf_preds, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1c515c-9619-4ef6-a667-26438f03b229",
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGBoost-Reg\n",
    "xgb_best_reg = XGBRegressor(colsample_bytree=0.8, gamma=0.5, learning_rate=0.01,\n",
    "                               max_depth=3, min_child_weight=6, n_estimators=1000,\n",
    "                        n_jobs=-1, objective='reg:squarederror', subsample=0.5,\n",
    "                        reg_alpha=10, reg_lambda=10,\n",
    "                        random_state=random_seed)\n",
    "\n",
    "xgb_best_reg.fit(X_train, y_train)\n",
    "\n",
    "xgb_preds_reg = xgb_best_reg.predict(X_test)\n",
    "\n",
    "eval_model(xgb_preds_reg,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab2e845-e370-4e68-9a69-efb1b69e7c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGBoost-Feat\n",
    "xgb_best= XGBRegressor(colsample_bytree=0.8, gamma=0.5, learning_rate=0.01,\n",
    "                               max_depth=3, min_child_weight=3, n_estimators=1000,\n",
    "                        n_jobs=-1, objective='reg:squarederror', subsample=0.8,\n",
    "                        random_state=random_seed)\n",
    "\n",
    "xgb_best.fit(X_train.loc[:,top_feats_ordered[:10]], y_train)\n",
    "\n",
    "xgb_preds = xgb_best.predict(X_test.loc[:,top_feats_ordered[:10]])\n",
    "\n",
    "eval_model(xgb_preds,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22cd0bb-1517-4dbb-87fb-b96048e47681",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Elastic Net\n",
    "enet_best = ElasticNet(alpha=0.1, l1_ratio=0.1)\n",
    "\n",
    "enet_best.fit(X_train, y_train)\n",
    "\n",
    "enet_preds = enet_best.predict(X_test)\n",
    "\n",
    "eval_model(enet_preds, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409a95e8-77b8-4c58-828e-92ad1e27579a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM - Linear\n",
    "svm_best = SVR(C=10, degree=2, gamma='auto', kernel='linear', epsilon=5)\n",
    "\n",
    "svm_best.fit(X_train_scaled.loc[:,top_feats_ordered[:10]], y_train)\n",
    "\n",
    "svm_preds = svm_best.predict(X_test_scaled.loc[:,top_feats_ordered[:10]])\n",
    "\n",
    "eval_model(svm_preds,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be67e3b-1c07-4f77-b665-48f54c1bcbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM - RBF\n",
    "svm_rbf = SVR(C=10,gamma='auto', kernel='rbf', epsilon=5)\n",
    "\n",
    "svm_rbf.fit(X_train_scaled.loc[:,top_feats_ordered[:10]], y_train)\n",
    "\n",
    "svm_rbf = svm_rbf.predict(X_test_scaled.loc[:,top_feats_ordered[:10]])\n",
    "\n",
    "eval_model(svm_rbf,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8515ea4a-0ec6-4963-aec1-23994207af8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM - Polynomial\n",
    "svm_poly = SVR(C=10, degree=3, gamma='auto', kernel='poly', epsilon=5)\n",
    "\n",
    "svm_poly.fit(X_train_scaled.loc[:,top_feats_ordered[:10]], y_train)\n",
    "\n",
    "svm_poly = svm_poly.predict(X_test_scaled.loc[:,top_feats_ordered[:10]])\n",
    "\n",
    "eval_model(svm_poly,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20be66c3-d5d1-448a-a8d4-8b4e65004189",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RuleFit\n",
    "rule_preds = rfit_enet.predict(X_test)\n",
    "\n",
    "eval_model(rule_preds, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fa3690-62e8-41e7-9bc5-b8fa517d809a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exporting Model Predictions for Williams Test\n",
    "pred_df = pd.DataFrame(columns=['pred_enet','pred_xgb','pred_svm','pred_rf','actual'])\n",
    "pred_df['pred_enet'] = enet_preds\n",
    "pred_df['pred_xgb'] = xgb_preds_reg\n",
    "pred_df['pred_svm'] = svm_preds\n",
    "pred_df['pred_rf'] = rf_preds\n",
    "pred_df['pred_rfit'] = rfit_preds_enet\n",
    "pred_df['actual'] = y_test\n",
    "pred_df.to_csv('all_preds.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
